# Qwen2.5-0.5B vLLM Server (GPU-Powered via Docker)

This project serves the **Qwen2.5-0.5B-Instruct** model using [vLLM](https://github.com/vllm-project/vllm) and a **FastAPI** backend inside a GPU-enabled Docker container. It supports **OpenAI-compatible chat completions**.

---

## ğŸš€ Features

- âœ… OpenAI-style endpoint: `/v1/chat/completions`
- âœ… GPU acceleration via CUDA 12.8
- âœ… Model loaded from local Hugging Face snapshot
- âœ… Docker & Docker Compose with NVIDIA runtime
- âœ… Health-check endpoint
- âœ… Pre-configured with `torch`, `transformers`, `vllm`, and FastAPI

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ Dockerfile               # Builds the container
â”œâ”€â”€ docker-compose.yml       # Runs the server with GPU
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ main.py                  # FastAPI + vLLM inference server
â”œâ”€â”€ download_model.py        # Downloads model from Hugging Face
â”œâ”€â”€ constants.py             # Local model path & model name
â”œâ”€â”€ Makefile                 # CLI commands for Docker tasks
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Steps

### Step 1 â€“ ğŸ”½ Download the Model (Optional if already mounted)

```bash
python download_model.py
```

> This downloads Qwen2.5-0.5B-Instruct to:
> `D:/JYN/EZ/EGITIM/LLM_Model_Registry/HuggingFaceRepo/app/models`

---

### Step 2 â€“ ğŸ³ Build & Run

You can use `make` or plain Docker:

```bash
# Using Makefile
make build     # Builds the Docker image
make up        # Starts the container
```

OR manually:

```bash
docker build -t vllm_service:latest .
docker-compose build --no-cache
docker-compose up
```

---

## ğŸ“¡ API Usage

### ğŸ”— Base URL

```
http://localhost:9999
```

### âœ… Endpoints

| Method | Path                  | Description              |
|--------|-----------------------|--------------------------|
| GET    | `/`                   | Root message             |
| GET    | `/health`             | Health + model status    |
| POST   | `/v1/chat/completions`| OpenAI-style chat endpoint |

---

### ğŸ§ª Example Request

**URL:**
```
POST http://localhost:9999/v1/chat/completions
```

**Request Body:**
```json
{
  "model": "Qwen/Qwen2.5-0.5B-Instruct",
  "messages": [
    {
      "role": "user",
      "content": "Hello, who are you?"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 512,
  "top_p": 0.9
}
```

---

## ğŸ–¥ï¸ Port Mapping

| Host Port | Container Port | Service         |
|-----------|----------------|------------------|
| `9999`    | `9999`         | FastAPI (Uvicorn) |

---

## ğŸ§  Environment Variables

Defined in `docker-compose.yml`:

- `MODEL_PATH=/app/models`
- `HF_HOME=/app/cache`
- `CUDA_VISIBLE_DEVICES=0`

---

## ğŸ”§ Makefile Commands

```bash
make build     # Build Docker image (docker-compose build)
make up        # Start the container (detached)
make down      # Stop and remove the container
make logs      # View real-time logs
make restart   # Restart container
make status    # Check running container status
```

---

## âœ… Health Check

```bash
curl http://localhost:9999/health
```

Expected response:
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

---

## ğŸ“œ License

MIT