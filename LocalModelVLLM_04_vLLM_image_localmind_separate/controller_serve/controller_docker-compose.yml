services:
  controller_llm_serving_api:
    build: .
    container_name: controller_llm_serving_api
    ports:
      - "9999:9999"
    environment:
      - LLM_API_HOST=http://vllm_openai_container:8000
    volumes:
      - ./logs:/app/logs
    networks:
      - llm_bridge

networks:
  llm_bridge:
    external: true