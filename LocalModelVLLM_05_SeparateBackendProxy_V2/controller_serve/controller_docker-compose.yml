services:
  controller_llm_serving_api:
    build: .
    container_name: controller_llm_serving_api
    ports:
      - "9999:9999"
    environment:
      - LLM_API_HOST=http://vllm_openai_container:8000
      - LOG_LEVEL=INFO
      - RATE_LIMIT_PER_MINUTE=100
      - BACKEND_TIMEOUT=60.0
    volumes:
      - ./logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9999/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - llm_bridge

networks:
  llm_bridge:
    external: true